---
phase: 01-infrastructure-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/config.py
  - backend/mcp_server.py
  - README.md
  - backend/app/api/prompts.py
  - backend/app/api/routes.py
autonomous: true
requirements:
  - INFRA-03
  - INFRA-05
  - INFRA-06

must_haves:
  truths:
    - "All timeout values and LLM settings are configurable via environment variables with sensible defaults"
    - "Codebase has zero references to removed MCP prediction modules or broken app.ml imports"
    - "README.md has no references to ML training, scikit-learn, or non-existent prediction endpoints"
    - "LLM safety settings use appropriate levels for F1 content instead of BLOCK_NONE everywhere"
  artifacts:
    - path: "backend/app/config.py"
      provides: "Centralized configuration constants"
      contains: "TOOL_TIMEOUT_SECONDS"
    - path: "backend/mcp_server.py"
      provides: "Clean MCP server without broken prediction tools"
    - path: "backend/README.md"
      provides: "Accurate documentation without ML/prediction references"
  key_links:
    - from: "backend/app/config.py"
      to: "backend/app/api/routes.py"
      via: "import of timeout and safety constants"
      pattern: "from app.config import"
    - from: "backend/app/api/routes.py"
      to: "backend/app/config.py"
      via: "safety settings and timeout values sourced from config"
      pattern: "TOOL_TIMEOUT_SECONDS|BLOCK_ONLY_HIGH"
---

<objective>
Create centralized configuration, remove dead MCP prediction code, and set appropriate LLM safety levels.

Purpose: Establish the config foundation that all subsequent plans import from, clean up broken dead code that obscures the real codebase, and harden LLM safety settings for F1-appropriate content filtering.
Output: New `backend/app/config.py` with all extracted constants, cleaned `mcp_server.py` and `README.md`, updated safety settings in `routes.py`.
</objective>

<execution_context>
@/Users/adityamurarka/.claude/get-shit-done/workflows/execute-plan.md
@/Users/adityamurarka/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-infrastructure-hardening/01-RESEARCH.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/CONCERNS.md

@backend/app/api/routes.py
@backend/app/api/tools.py
@backend/mcp_server.py
@backend/main.py
@backend/README.md
@backend/app/api/prompts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create centralized config.py and wire up all hardcoded values</name>
  <files>backend/app/config.py, backend/app/api/routes.py, backend/main.py</files>
  <action>
Create NEW file `backend/app/config.py` with all magic numbers extracted from the codebase as environment-configurable constants with sensible defaults. Use `os.getenv()` pattern.

Constants to extract (with current hardcoded locations):
- TOOL_TIMEOUT_SECONDS = 30 (routes.py:160)
- FASTF1_TIMEOUT_SECONDS = 60 (routes.py:622)
- OPENF1_HTTP_TIMEOUT_SECONDS = 10 (routes.py:670, 710)
- WS_RECEIVE_TIMEOUT = 0.1 (routes.py:887)
- WS_HEARTBEAT_INTERVAL = 15 (NEW, for Plan 02)
- WS_STALE_TIMEOUT = 60 (NEW, for Plan 02)
- WS_POLL_INTERVAL = 8 (NEW, for Plan 02)
- MAX_AGENT_TURNS = 5 (routes.py:129)
- PREFETCH_STARTUP_DELAY = 30 (main.py:39)
- PREFETCH_RACE_TIMEOUT_SECONDS = 60 (main.py:76)
- PREFETCH_INTER_RACE_DELAY = 5 (main.py:88)
- PREFETCH_INTERVAL = 1800 (main.py:93)
- CHROMA_DB_PATH = "data/chroma" (tools.py - used per call currently)
- EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2" (tools.py)
- RULEBOOK_TOP_K = 6 (tools.py)
- LLM_MODEL_NAME = "gemini-2.0-flash" (routes.py)
- LLM_TEMPERATURE = 0 (routes.py)

Then update `routes.py` and `main.py` to import from `app.config` instead of using hardcoded values. Replace each magic number with its named constant. Do NOT yet replace print() statements (that is Plan 02). Do NOT yet refactor ChromaDB or WebSocket (that is Plan 02).

In `routes.py`, also update the safety_settings dict to import constant values from config for the model name and temperature.
  </action>
  <verify>
Run `cd /Users/adityamurarka/Desktop/F1-AI/backend && python -c "from app.config import TOOL_TIMEOUT_SECONDS, FASTF1_TIMEOUT_SECONDS, MAX_AGENT_TURNS; print('Config OK')"` succeeds.
Run `grep -rn "timeout=30\b" backend/app/api/routes.py` returns zero matches (hardcoded 30s timeout replaced).
Run `grep -rn "sleep(1800)" backend/main.py` returns zero matches (hardcoded prefetch interval replaced).
  </verify>
  <done>All hardcoded timeout values, model names, and configuration constants are extracted to backend/app/config.py and imported where previously hardcoded. No magic numbers remain in routes.py or main.py for the identified values.</done>
</task>

<task type="auto">
  <name>Task 2: Remove dead MCP prediction stubs and clean README</name>
  <files>backend/mcp_server.py, backend/README.md, backend/app/api/prompts.py</files>
  <action>
In `backend/mcp_server.py`:
- Remove the `predict_race_results` tool function (around lines 497-512) which imports from non-existent `app.ml.predict`
- Remove the `calculate_championship_scenario` tool function (around lines 514-525) which imports from non-existent `app.ml.scenario`
- Verify no other references to `app.ml` remain in the file

In `backend/README.md`:
- Remove "ML model trained on historical race data" reference (around line 6)
- Remove `scikit-learn (ML race predictions)` from architecture diagram (around line 31)
- Rewrite Race Predictions feature row to say "statistical/heuristic approach" instead of ML (around line 42)
- Remove the entire ML model training section (around lines 109-117)
- Remove `/api/predictions` endpoint reference (around line 135) since it does not exist in routes.py
- Remove MCP tools 11/12 (predict, scenario) references (around lines 199-200)
- Remove ML directory references (around lines 302, 319-320)
- Remove scikit-learn from tech stack listing (around line 379)
- Keep predictions page reference if it exists (frontend will be reimplemented in Phase 2)

In `backend/app/api/prompts.py`:
- Check for any references to ML prediction capabilities and remove them
- Ensure the system prompt does not claim the bot can do ML-based predictions (it will use statistical approach in Phase 2)

Use `grep -rn "app\.ml\|scikit\|joblib\|predict_race\|calculate_championship_scenario\|ML model\|ML training" backend/` to verify all dead references are gone after changes.
  </action>
  <verify>
Run `grep -rn "app\.ml" backend/` returns zero matches.
Run `grep -rn "scikit" backend/` returns zero matches.
Run `grep -rn "predict_race_results\|calculate_championship_scenario" backend/mcp_server.py` returns zero matches.
Run `cd /Users/adityamurarka/Desktop/F1-AI/backend && python -c "import mcp_server; print('MCP import OK')"` does not crash on broken imports.
  </verify>
  <done>mcp_server.py has no prediction tool stubs or broken app.ml imports. README.md has no ML/scikit-learn/prediction endpoint references. prompts.py does not claim ML prediction capability.</done>
</task>

<task type="auto">
  <name>Task 3: Set appropriate LLM safety settings for F1 domain</name>
  <files>backend/app/api/routes.py</files>
  <action>
In `backend/app/api/routes.py`, update the safety_settings dict (around lines 50-55) from BLOCK_NONE on all categories to F1-appropriate levels:

- HARM_CATEGORY_DANGEROUS_CONTENT: BLOCK_ONLY_HIGH (F1 discusses crashes, fires, injuries)
- HARM_CATEGORY_HARASSMENT: BLOCK_ONLY_HIGH (F1 has team rivalries, driver criticism)
- HARM_CATEGORY_HATE_SPEECH: BLOCK_MEDIUM_AND_ABOVE (not relevant to F1, can be stricter)
- HARM_CATEGORY_SEXUALLY_EXPLICIT: BLOCK_MEDIUM_AND_ABOVE (not relevant to F1, can be stricter)

Add a comment block above the dict explaining WHY each level was chosen (the F1 content justification). The system prompt in prompts.py already has strong identity guardrails that refuse non-F1 topics, so this is defense-in-depth.

Per research: Do NOT use BLOCK_MEDIUM_AND_ABOVE for DANGEROUS_CONTENT because F1 content legitimately discusses crash scenarios (e.g., "the crash at Copse", "driver hospitalization").
  </action>
  <verify>
Run `grep -n "BLOCK_NONE" backend/app/api/routes.py` returns zero matches (all BLOCK_NONE removed).
Run `grep -c "BLOCK_ONLY_HIGH\|BLOCK_MEDIUM_AND_ABOVE" backend/app/api/routes.py` returns 4 (one per category).
  </verify>
  <done>Safety settings use BLOCK_ONLY_HIGH for dangerous content and harassment (F1-appropriate), BLOCK_MEDIUM_AND_ABOVE for hate speech and sexually explicit (stricter). No BLOCK_NONE remains.</done>
</task>

</tasks>

<verification>
1. `python -c "from app.config import TOOL_TIMEOUT_SECONDS; print(TOOL_TIMEOUT_SECONDS)"` prints 30 (default)
2. `grep -rn "BLOCK_NONE" backend/app/api/routes.py` returns nothing
3. `grep -rn "app\.ml" backend/` returns nothing
4. `grep -rn "scikit" backend/README.md` returns nothing
5. Backend starts without import errors: `cd backend && timeout 10 python -c "from app.api.routes import router; print('Routes OK')"`
</verification>

<success_criteria>
- backend/app/config.py exists with all extracted constants
- All hardcoded timeouts in routes.py and main.py replaced with config imports
- mcp_server.py has zero broken prediction tool stubs
- README.md has zero ML/scikit-learn/prediction module references
- LLM safety settings use appropriate non-BLOCK_NONE levels
- Backend imports succeed without errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-infrastructure-hardening/01-01-SUMMARY.md`
</output>
