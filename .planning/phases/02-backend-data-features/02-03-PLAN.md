---
phase: 02-backend-data-features
plan: 03
type: execute
wave: 3
depends_on: [02-01, 02-02]
files_modified:
  - backend/app/api/tools.py
  - backend/app/api/routes.py
  - backend/app/api/prompts.py
autonomous: true
requirements: [DATA-04, DATA-05]

must_haves:
  truths:
    - "Asking the chat 'Who will win the next race?' invokes get_race_predictions tool and returns probabilistic analysis with all 20 drivers"
    - "Asking the chat about pit strategy invokes get_pit_strategy tool and returns undercut/overcut analysis with stint data"
    - "Asking the chat about weather at a circuit invokes get_weather_conditions tool and returns real weather data (replacing the old stub)"
    - "GET /api/predictions/{year}/{round_num} returns structured JSON with predictions array, metadata, and accuracy stats"
    - "Predictions REST endpoint caches results until new data arrives"
  artifacts:
    - path: "backend/app/api/tools.py"
      provides: "Three new LangChain @tool functions wrapping data modules"
      contains: "get_race_predictions"
    - path: "backend/app/api/routes.py"
      provides: "GET /api/predictions/{year}/{round_num} REST endpoint"
      contains: "predictions"
    - path: "backend/app/api/prompts.py"
      provides: "Updated system prompt mentioning prediction, strategy, and weather capabilities"
      contains: "predict"
  key_links:
    - from: "backend/app/api/tools.py"
      to: "backend/app/data/predictions.py"
      via: "import compute_race_predictions"
      pattern: "from app.data.predictions import"
    - from: "backend/app/api/tools.py"
      to: "backend/app/data/strategy.py"
      via: "import analyze_pit_strategy"
      pattern: "from app.data.strategy import"
    - from: "backend/app/api/tools.py"
      to: "backend/app/data/weather.py"
      via: "import get_weather_for_circuit"
      pattern: "from app.data.weather import"
    - from: "backend/app/api/routes.py"
      to: "backend/app/data/predictions.py"
      via: "import for REST endpoint"
      pattern: "from app.data.predictions import"
    - from: "backend/app/api/tools.py"
      to: "TOOL_LIST"
      via: "registered in tool list and map"
      pattern: "TOOL_LIST.*get_race_predictions"
---

<objective>
Wire the prediction, strategy, and weather modules as LangChain tools for the agentic chat, add the predictions REST endpoint, and update the system prompt.

Purpose: DATA-04 requires all three modules to be callable as LangChain tools. DATA-05 requires the REST endpoint for iOS/web consumption. This plan connects the computation modules from Plans 01-02 to the existing agentic loop and API surface.

Output: Three new @tool functions in tools.py, GET /api/predictions/{year}/{round_num} endpoint in routes.py, updated system prompt in prompts.py.
</objective>

<execution_context>
@/Users/adityamurarka/.claude/get-shit-done/workflows/execute-plan.md
@/Users/adityamurarka/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-backend-data-features/02-CONTEXT.md
@.planning/phases/02-backend-data-features/02-RESEARCH.md
@.planning/phases/02-backend-data-features/02-01-SUMMARY.md
@.planning/phases/02-backend-data-features/02-02-SUMMARY.md
@backend/app/api/tools.py
@backend/app/api/routes.py
@backend/app/api/prompts.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add three LangChain tool wrappers and update system prompt</name>
  <files>
    backend/app/api/tools.py
    backend/app/api/prompts.py
  </files>
  <action>
**Part A: Add tool functions to tools.py**

Add three new @tool functions to `backend/app/api/tools.py`, following the existing pattern (decorator, docstring, structured return):

1. **`get_race_predictions(year: int, round_num: int)`** — Wraps `compute_race_predictions()` from `app.data.predictions`. The chat version should be RICHER than the REST API per CONTEXT.md locked decision:
   - Call `compute_race_predictions(year, round_num)` to get structured data
   - Format as a race-engineer briefing string for the chat: include narrative reasoning, driver-by-driver analysis for top 5, summary table for positions 6-20, confidence ranges in "72-85% confidence" format, and accuracy stats
   - Include race-engineer personality: "Based on my analysis of qualifying pace and recent form..."
   - Docstring should guide the LLM: "Predicts race finishing order for all 20 drivers with confidence ranges and reasoning factors. Use when user asks about race predictions, who will win, expected race results, or finishing order."

2. **`get_pit_strategy(year: int, round_num: int, driver_code: str = None)`** — Wraps `analyze_pit_strategy()` from `app.data.strategy`.
   - Call `analyze_pit_strategy(year, round_num, driver_code)`
   - Format stint data as readable text with compound emojis or labels, lap ranges, degradation info
   - Include historical context and safety car probability
   - Docstring: "Analyzes pit strategy including tyre stints, undercut/overcut opportunities, and historical strategy data. Use when user asks about pit strategy, tyre choices, undercut, overcut, or stint analysis."

3. **`get_weather_conditions(location: str)`** — REPLACES the existing `get_track_conditions` stub. Wraps `get_weather_for_circuit()` from `app.data.weather`.
   - REMOVE the old `get_track_conditions` function entirely
   - The new function should have the SAME name pattern but call real weather data
   - Since `get_weather_for_circuit` is async, use `asyncio.get_event_loop().run_until_complete()` or define the tool as sync and use `asyncio.run()` in a new event loop (check what works within LangChain tool context — if tools are called from sync context in the agentic loop, create a new event loop). Alternatively, if the existing agentic loop calls tools synchronously, make the weather fetch synchronous using `httpx.Client` instead of `httpx.AsyncClient` within the tool wrapper.
   - Format weather data as readable text: current conditions, hourly forecast summary, track context, strategy impact
   - Docstring: "Fetches current weather and hourly forecast for an F1 circuit. Use when user asks about weather conditions, rain probability, track temperature, or how weather affects strategy."

**Part B: Update TOOL_LIST and TOOL_MAP**

Replace `get_track_conditions` with `get_weather_conditions` in TOOL_LIST. Add `get_race_predictions` and `get_pit_strategy` to TOOL_LIST. TOOL_MAP auto-rebuilds from the list.

**Part C: Update system prompt in prompts.py**

Update `RACE_ENGINEER_PERSONA` in `backend/app/api/prompts.py` to mention the new capabilities:
- Add: "You can predict race outcomes with confidence ranges based on qualifying data, recent form, and circuit history"
- Add: "You can analyze pit strategy including tyre stints, degradation, undercut/overcut scenarios, and historical strategy at any circuit"
- Add: "You can provide real-time weather data for any F1 circuit including temperature, rain probability, wind, and how it affects race strategy"
- Remove or update any reference to weather being unavailable (since it's no longer a stub)

Use structlog for logging in all new tool functions.
  </action>
  <verify>
    Run: `cd /Users/adityamurarka/Desktop/F1-AI/backend && python -c "from app.api.tools import TOOL_LIST, TOOL_MAP; names = [t.name for t in TOOL_LIST]; print(f'Tools: {names}'); assert 'get_race_predictions' in names; assert 'get_pit_strategy' in names; assert 'get_weather_conditions' in names; assert 'get_track_conditions' not in names; print('tool registry OK')"`
    Run: `cd /Users/adityamurarka/Desktop/F1-AI/backend && python -c "from app.api.prompts import RACE_ENGINEER_PERSONA; assert 'predict' in RACE_ENGINEER_PERSONA.lower(); print('prompt updated OK')"`
  </verify>
  <done>
    - Three new @tool functions registered in TOOL_LIST and TOOL_MAP
    - Old get_track_conditions stub is removed and replaced by get_weather_conditions
    - get_race_predictions formats rich chat output with race-engineer personality and narrative reasoning
    - get_pit_strategy formats stint data with historical context
    - get_weather_conditions calls real weather API instead of returning stub text
    - System prompt mentions prediction, strategy, and weather capabilities
    - Tool docstrings guide the LLM on when to invoke each tool
  </done>
</task>

<task type="auto">
  <name>Task 2: Add predictions REST endpoint with caching</name>
  <files>
    backend/app/api/routes.py
  </files>
  <action>
Add `GET /api/predictions/{year}/{round_num}` endpoint to `backend/app/api/routes.py`:

1. **Import** `compute_race_predictions` from `app.data.predictions` and `FASTF1_TIMEOUT_SECONDS` from `app.config`.

2. **Cache** — Add module-level dict: `predictions_cache: dict[tuple[int, int], dict] = {}`. This matches the existing `race_detail_cache` pattern already in routes.py.

3. **Endpoint implementation:**
```python
@router.get("/predictions/{year}/{round_num}")
async def get_predictions_endpoint(year: int, round_num: int):
    """Structured race predictions for iOS and web consumption.

    Returns all 20 drivers with predicted positions, confidence ranges,
    reasoning factors, and model accuracy statistics.

    Per CONTEXT.md: REST version returns structured numbers + factors.
    Chat version (via tool) adds narrative reasoning and personality.
    """
    cache_key = (year, round_num)

    if cache_key in predictions_cache:
        cached = predictions_cache[cache_key]
        # Return cached unless we detect new qualifying data is available
        # (simple heuristic: if cached data_sources doesn't include "qualifying"
        # but qualifying is now available, recompute)
        if not _should_recompute_predictions(year, round_num, cached):
            return cached

    try:
        result = await asyncio.wait_for(
            asyncio.to_thread(compute_race_predictions, year, round_num),
            timeout=FASTF1_TIMEOUT_SECONDS,
        )
        predictions_cache[cache_key] = result
        return result
    except asyncio.TimeoutError:
        logger.error("predictions.timeout", year=year, round_num=round_num)
        return {"error": "Prediction computation timed out", "year": year, "round": round_num}
    except Exception as e:
        logger.error("predictions.error", year=year, round_num=round_num, error=str(e))
        return {"error": f"Failed to compute predictions: {str(e)}", "year": year, "round": round_num}
```

4. **Cache invalidation helper:**
```python
def _should_recompute_predictions(year: int, round_num: int, cached: dict) -> bool:
    """Return True if cached prediction should be recomputed.

    Recompute when qualifying data becomes available but cached prediction
    was generated from practice/historical data only.
    """
    cached_sources = cached.get("data_sources", [])
    if "qualifying" in cached_sources:
        return False  # Already has qualifying data, no need to recompute

    # Check if qualifying data is now available by trying to load it
    # This is a lightweight check — just see if the session exists
    try:
        import fastf1
        session = fastf1.get_session(year, round_num, "Q")
        # If we can get the session object without error, qualifying happened
        return True
    except Exception:
        return False
```

5. **Response format** — The `compute_race_predictions()` function already returns the correct REST-ready dict (flat array sorted by predicted position with metadata envelope). The endpoint returns it directly. Per CONTEXT.md locked decision: "Include full metadata: timestamp, data sources used, model accuracy stats".

6. **Error responses** — Return JSON error objects (not HTTP 500) so iOS/web clients can display meaningful messages. Use structlog for error logging.

Use structlog throughout (already imported in routes.py).
  </action>
  <verify>
    Run: `cd /Users/adityamurarka/Desktop/F1-AI/backend && python -c "from app.api.routes import router; routes = [r.path for r in router.routes]; print(f'Routes: {routes}'); assert '/predictions/{year}/{round_num}' in routes; print('endpoint registered OK')"`
  </verify>
  <done>
    - GET /api/predictions/{year}/{round_num} endpoint is registered on the router
    - Endpoint returns structured JSON with predictions array, metadata (timestamp, data_sources), and accuracy stats
    - Response uses asyncio.to_thread() with timeout for FastF1 data loading (non-blocking)
    - Predictions are cached in-memory by (year, round_num) tuple
    - Cache invalidation detects when qualifying data becomes available and triggers recompute
    - Error responses return JSON objects (not 500 errors) with meaningful messages
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.api.tools import TOOL_LIST; print([t.name for t in TOOL_LIST])"` shows 13 tools including the 3 new ones
2. `get_track_conditions` is NOT in TOOL_LIST (replaced by get_weather_conditions)
3. `python -c "from app.api.routes import router; print([r.path for r in router.routes])"` includes `/predictions/{year}/{round_num}`
4. System prompt in prompts.py references prediction, strategy, and weather capabilities
5. Tool docstrings clearly describe when each tool should be used
6. REST endpoint uses asyncio.to_thread with timeout pattern
</verification>

<success_criteria>
- Three new tools (predictions, strategy, weather) are callable by the agentic chat loop
- Old weather stub is fully replaced — no more "Live weather data is not yet available" response
- REST endpoint returns structured predictions matching the response shape from RESEARCH.md
- Predictions are cached and only recomputed when new qualifying data arrives
- System prompt accurately describes all available capabilities
- Chat predictions include richer narrative reasoning than REST response (per locked decision)
</success_criteria>

<output>
After completion, create `.planning/phases/02-backend-data-features/02-03-SUMMARY.md`
</output>
