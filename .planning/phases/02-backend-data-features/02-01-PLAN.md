---
phase: 02-backend-data-features
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/data/__init__.py
  - backend/app/data/predictions.py
  - backend/app/config.py
autonomous: true
requirements: [DATA-01]

must_haves:
  truths:
    - "compute_race_predictions(year, round_num) returns all 20 drivers with predicted positions, confidence percentage ranges, and top 3 reasoning factors"
    - "Pre-qualifying fallback uses practice session data + historical circuit data with wider confidence ranges when qualifying hasn't happened yet"
    - "Prediction accuracy tracker compares past predictions to actual results and reports top-3 accuracy percentage and average position error"
    - "Predictions cache aggressively — historical data loaded once per (circuit, season) pair, qualifying data cached per (year, round)"
  artifacts:
    - path: "backend/app/data/__init__.py"
      provides: "Data module package init"
    - path: "backend/app/data/predictions.py"
      provides: "Race prediction engine with heuristic scoring and accuracy tracking"
      contains: "compute_race_predictions"
    - path: "backend/app/config.py"
      provides: "Prediction scoring weights and OpenWeatherMap API key config"
      contains: "QUALIFYING_WEIGHT"
  key_links:
    - from: "backend/app/data/predictions.py"
      to: "fastf1"
      via: "session.load() with _fastf1_lock for thread safety"
      pattern: "_fastf1_lock"
    - from: "backend/app/data/predictions.py"
      to: "backend/app/config.py"
      via: "import scoring weight constants"
      pattern: "from app.config import"
---

<objective>
Build the race prediction engine that computes probabilistic race outcome predictions for all 20 drivers using a weighted heuristic scoring model.

Purpose: DATA-01 requires predictions based on qualifying data, historical results, and track characteristics with confidence ranges and reasoning factors. This is the heaviest computation module in Phase 2 and the foundation for both the chat tool and REST endpoint.

Output: `backend/app/data/predictions.py` with `compute_race_predictions()` function, accuracy tracker, and prediction caching. Config constants for scoring weights.
</objective>

<execution_context>
@/Users/adityamurarka/.claude/get-shit-done/workflows/execute-plan.md
@/Users/adityamurarka/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-backend-data-features/02-CONTEXT.md
@.planning/phases/02-backend-data-features/02-RESEARCH.md
@backend/app/config.py
@backend/app/api/tools.py
@backend/app/api/routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create prediction scoring engine with heuristic model</name>
  <files>
    backend/app/data/__init__.py
    backend/app/data/predictions.py
    backend/app/config.py
  </files>
  <action>
Create `backend/app/data/__init__.py` (empty or minimal docstring).

Create `backend/app/data/predictions.py` implementing:

1. **Scoring weights in config.py** — Add to `backend/app/config.py`:
   - `QUALIFYING_WEIGHT = float(os.getenv("QUALIFYING_WEIGHT", "0.35"))`
   - `RECENT_FORM_WEIGHT = float(os.getenv("RECENT_FORM_WEIGHT", "0.25"))`
   - `CIRCUIT_HISTORY_WEIGHT = float(os.getenv("CIRCUIT_HISTORY_WEIGHT", "0.20"))`
   - `TEAM_STRENGTH_WEIGHT = float(os.getenv("TEAM_STRENGTH_WEIGHT", "0.15"))`
   - `GRID_TO_FINISH_WEIGHT = float(os.getenv("GRID_TO_FINISH_WEIGHT", "0.05"))`
   - `OPENWEATHERMAP_API_KEY = os.getenv("OPENWEATHERMAP_API_KEY", "")` (needed by weather in Plan 02, add now to avoid config.py conflicts)
   - `WEATHER_CACHE_TTL = int(os.getenv("WEATHER_CACHE_TTL", "600"))` (10-minute TTL per Claude's discretion)
   - `PREDICTION_HISTORY_PATH = os.getenv("PREDICTION_HISTORY_PATH", "data/prediction_history.json")`

2. **FastF1 data loading with thread safety** — Use a module-level `_fastf1_lock = threading.Lock()` (same pattern as tools.py). All `fastf1.get_session().load()` calls wrapped with this lock. Load only what's needed: `telemetry=False, laps=False, weather=False` for qualifying results; `laps=True` only when stint data is needed.

3. **Data caching** — In-memory dicts keyed by `(year, round_num)` for qualifying results, `(circuit_key, year)` for circuit history, `(driver_code,)` for recent form. Cache persists across requests within same server process.

4. **`compute_race_predictions(year: int, round_num: int) -> dict`** — Main function:
   - Load qualifying results for this round (or practice data if qualifying hasn't happened — detect by catching exceptions or checking session availability)
   - Load last 5 race results for each driver (recent form)
   - Load driver history at this specific circuit (last 3 editions per CONTEXT.md)
   - Load constructor championship standings (team strength)
   - For each of 20 drivers, compute weighted score: `QUALIFYING_WEIGHT * quali_pos + RECENT_FORM_WEIGHT * avg(recent_results) + CIRCUIT_HISTORY_WEIGHT * avg(circuit_results) + TEAM_STRENGTH_WEIGHT * team_pos + GRID_TO_FINISH_WEIGHT * historical_grid_delta`
   - Sort by score (lower = better predicted position)
   - Compute confidence ranges: when data signals agree (low variance), confidence is tighter (e.g., 75-88%); when they conflict, confidence is wider (e.g., 45-65%). Use std_dev of normalized inputs.
   - Generate top 3 reasoning factors per driver from the dominant scoring components (e.g., "Pole position (qualifying P1)", "Won 3 of last 5 races", "2nd at Monaco 2024")
   - Return dict matching the REST response shape from RESEARCH.md: `{"year", "round", "grand_prix", "generated_at", "data_sources", "accuracy", "predictions": [{"position", "driver_code", "driver_name", "team", "confidence_low", "confidence_high", "factors": [...]}], "weather_impact", "wet_scenario"}`

5. **Pre-qualifying fallback** — When qualifying session is not available:
   - Try loading FP1/FP2/FP3 data (best lap times as proxy for pace)
   - Fall back to championship position + circuit history only
   - Label output with `"data_sources": ["practice", "circuit_history", "championship_position"]` instead of `["qualifying", ...]`
   - Widen all confidence ranges by 15 percentage points (more uncertainty without qualifying data)
   - Use practice data as a weak signal with ~0.10 weight per RESEARCH.md recommendation

6. **Robust error handling** — Use structlog throughout. If FastF1 data loading fails for a specific session, skip that data source and adjust weights proportionally. Never crash the entire prediction on one missing data point. Return partial predictions with a `"warnings"` field listing what data was unavailable.

Use structlog for all logging (import pattern: `logger = structlog.get_logger()`).
  </action>
  <verify>
    Run: `cd /Users/adityamurarka/Desktop/F1-AI/backend && python -c "from app.data.predictions import compute_race_predictions; print('import OK')"`
    Run: `cd /Users/adityamurarka/Desktop/F1-AI/backend && python -c "from app.config import QUALIFYING_WEIGHT, RECENT_FORM_WEIGHT, CIRCUIT_HISTORY_WEIGHT, TEAM_STRENGTH_WEIGHT, GRID_TO_FINISH_WEIGHT, OPENWEATHERMAP_API_KEY, WEATHER_CACHE_TTL, PREDICTION_HISTORY_PATH; print('config OK')"`
  </verify>
  <done>
    - `compute_race_predictions` is importable and accepts (year, round_num)
    - All 5 scoring weight constants + weather/prediction config are in config.py with os.getenv() pattern
    - Function returns dict with all 20 drivers, predicted positions, confidence_low/confidence_high as percentages, and factors list (3 items per driver)
    - Pre-qualifying fallback path exists (detects missing qualifying, uses practice/historical data)
    - Data caching dicts prevent redundant FastF1 session loads
  </done>
</task>

<task type="auto">
  <name>Task 2: Add prediction accuracy tracker with JSON persistence</name>
  <files>
    backend/app/data/predictions.py
  </files>
  <action>
Add accuracy tracking to `backend/app/data/predictions.py`:

1. **`save_prediction(year: int, round_num: int, predictions: dict)`** — Saves a prediction to `PREDICTION_HISTORY_PATH` (JSON file). Structure: `{"(year,round)": {"predicted_positions": {"VER": 1, "HAM": 2, ...}, "generated_at": "...", "actual_positions": null}}`. Create the file if it doesn't exist. Use a file lock or simple atomic write pattern to avoid corruption.

2. **`record_actual_result(year: int, round_num: int)`** — After a race completes, loads actual finishing positions from FastF1 and stores them in the prediction history entry. Called lazily when accuracy stats are requested.

3. **`get_accuracy_stats(last_n_races: int = 5) -> dict`** — Computes rolling accuracy over the last N races where both prediction and actual results exist:
   - `top3_accuracy_pct`: What percentage of top-3 predictions were correct (predicted in top 3 AND finished in top 3)?
   - `top10_accuracy_pct`: What percentage of predicted top-10 finished in actual top-10?
   - `avg_position_error`: Mean absolute difference between predicted and actual positions across all drivers
   - `races_evaluated`: How many races had both prediction and actual data
   - Return `{"recent_top3_pct": 78, "recent_top10_pct": 65, "avg_position_error": 2.3, "races_evaluated": 4}`

4. **Integration with compute_race_predictions** — At the end of `compute_race_predictions()`, call `save_prediction()` to store the prediction, and populate the `"accuracy"` field in the return dict from `get_accuracy_stats()`.

5. **Graceful degradation** — If JSON file is corrupted or missing, return `{"races_evaluated": 0}` and recreate the file. Never crash predictions because accuracy tracking fails.

Use structlog for all logging.
  </action>
  <verify>
    Run: `cd /Users/adityamurarka/Desktop/F1-AI/backend && python -c "from app.data.predictions import save_prediction, get_accuracy_stats; print('accuracy tracker OK')"`
  </verify>
  <done>
    - `save_prediction()` writes prediction data to JSON file at PREDICTION_HISTORY_PATH
    - `get_accuracy_stats()` returns dict with top3_accuracy_pct, top10_accuracy_pct, avg_position_error, races_evaluated
    - `compute_race_predictions()` includes accuracy stats in its return value under the "accuracy" key
    - Corrupted or missing JSON file does not crash the system — graceful fallback to empty stats
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.data.predictions import compute_race_predictions, save_prediction, get_accuracy_stats"` succeeds
2. `python -c "from app.config import QUALIFYING_WEIGHT, WEATHER_CACHE_TTL, PREDICTION_HISTORY_PATH"` succeeds
3. `backend/app/data/__init__.py` exists
4. `backend/app/data/predictions.py` contains `compute_race_predictions`, `save_prediction`, `get_accuracy_stats`
5. Config.py has all 8 new constants with os.getenv() pattern
</verification>

<success_criteria>
- Prediction engine computes weighted scores for all 20 drivers from qualifying + recent form + circuit history + team strength
- Confidence ranges expressed as percentage pairs (not tier labels) that widen when data signals conflict
- Top 3 reasoning factors generated per driver from dominant scoring components
- Pre-qualifying fallback detects missing qualifying and uses practice data with wider confidence
- Accuracy tracker persists predictions to JSON and computes rolling accuracy stats
- All FastF1 calls use thread-safe locking pattern
</success_criteria>

<output>
After completion, create `.planning/phases/02-backend-data-features/02-01-SUMMARY.md`
</output>
